{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, auc, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Define functions to read TSV files\n",
    "def read_tsv(filename, inf_ind, skip_1st=False, file_encoding=\"utf8\"):\n",
    "    extract_inf = []\n",
    "    with open(filename, \"r\", encoding=file_encoding) as tsv_f:\n",
    "        if skip_1st:\n",
    "            tsv_f.readline()\n",
    "        line = tsv_f.readline()\n",
    "        while line:\n",
    "            line_list = line.strip().split(\"\\t\")\n",
    "            temp_inf = [line_list[ind] for ind in inf_ind]\n",
    "            extract_inf.append(temp_inf)\n",
    "            line = tsv_f.readline()\n",
    "    return extract_inf\n",
    "\n",
    "# Read amino acid feature files and generate feature dictionaries\n",
    "def get_features(filename, f_num=15):\n",
    "    f_list = read_tsv(filename, list(range(16)), True)\n",
    "    f_dict = {}\n",
    "    left_num = 0\n",
    "    right_num = 0\n",
    "    if f_num > 15:\n",
    "        left_num = (f_num - 15) // 2\n",
    "        right_num = f_num - 15 - left_num\n",
    "    for f in f_list:\n",
    "        f_dict[f[0]] = [0] * left_num + [float(x) for x in f[1:]] + [0] * right_num\n",
    "    f_dict[\"X\"] = [0] * f_num\n",
    "    return f_dict\n",
    "\n",
    "# Defining Input Functions\n",
    "def generate_input(sps, sp_lbs, feature_dict, feature_num, ins_num, max_len):\n",
    "    xs, ys, lens = [], [], []\n",
    "\n",
    "    for i, sp in enumerate(sps):\n",
    "        ys.append(sp_lbs[i])\n",
    "        lens.extend([len(tcr[0]) if tcr[0] else 0 for tcr in sp])\n",
    "\n",
    "    while len(lens) % ins_num != 0:\n",
    "        lens = np.concatenate((lens, np.array([0]))) \n",
    "    lens = np.array(lens)\n",
    "    lens = lens.reshape(-1, ins_num)\n",
    "    while lens.shape[0] < len(sps):\n",
    "        lens = np.concatenate((lens, np.zeros((1, ins_num))), axis=0)\n",
    "    for i, sp in enumerate(sps):\n",
    "        x = [[[0] * feature_num for _ in range(max_len)] for _ in range(ins_num)]\n",
    "        seq_count = 0  \n",
    "        for j, tcr in enumerate(sp):\n",
    "            tcr_seq = tcr[0]\n",
    "            right_num = max_len - len(tcr_seq)\n",
    "            tcr_seq += \"X\" * right_num\n",
    "            tcr_matrix = []\n",
    "            for aa in tcr_seq:\n",
    "                tcr_matrix.append(feature_dict[aa.upper()])\n",
    "            x[seq_count] = tcr_matrix\n",
    "            seq_count += 1\n",
    "        xs.append(x)\n",
    "\n",
    "\n",
    "    xs = np.array(xs)\n",
    "    xs = torch.tensor(xs, dtype=torch.float32)\n",
    "    xs = xs.swapaxes(2, 3)\n",
    "    ys = np.array(ys)\n",
    "    ys = torch.tensor(ys, dtype=torch.float32).view(-1, 1)\n",
    "    lens = torch.tensor(lens, dtype=torch.long)\n",
    "\n",
    "    return xs, ys, lens\n",
    "\n",
    "\n",
    "#Define the Generate Label function\n",
    "def load_data(sample_dir):\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    for sample_file in os.listdir(sample_dir):\n",
    "        training_data.append(read_tsv(os.path.join(sample_dir, sample_file), [0, 1], True))\n",
    "        if \"P\" in sample_file:\n",
    "            training_labels.append(1)\n",
    "        elif \"H\" in sample_file:\n",
    "            training_labels.append(0)\n",
    "        else:\n",
    "            print(\"Wrong sample filename! Please name positive samples with 'P' and negative samples with 'H'.\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "    return training_data, training_labels\n",
    "\n",
    "\n",
    "#Define the evaluation function\n",
    "from tqdm import tqdm\n",
    "def evaluate(model, criterion, test_loader, device='cuda'):\n",
    "    test_total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_batch_x, test_batch_y, test_valid_lens in test_loader:\n",
    "            test_batch_x = test_batch_x.view(-1, 24, 15).to(device)\n",
    "            test_batch_y = test_batch_y.to(device)\n",
    "            test_pred = model(test_batch_x, test_valid_lens)\n",
    "\n",
    "            test_loss = criterion(test_pred, test_batch_y)\n",
    "            test_total_loss += test_loss.item()\n",
    "            all_preds.append(test_pred.cpu().numpy())\n",
    "            all_labels.append(test_batch_y.cpu().numpy())\n",
    "            \n",
    "        test_avg_loss = test_total_loss / len(test_loader)\n",
    "        return test_avg_loss, all_preds, all_labels\n",
    "    \n",
    "#Define the training function \n",
    "def train(fold, model, criterion, optimizer, train_loader, test_loader, epoches=100, device='cuda'):\n",
    "    \n",
    "    model_path = f'../model/AutoBILSTMTFCNY/{disease_name}checkpoint{fold}.pt'  # Save path and naming of model files\n",
    "    early_stopping = EarlyStopping(PATIENCE, path=model_path, verbose=False)\n",
    "    \n",
    "\n",
    "    epoch_train_losses = []\n",
    "    epoch_test_losses = []\n",
    "    with tqdm(total=epoches) as t:\n",
    "        t.set_description(f'{disease_name} - Fold {fold}')\n",
    "        for epoch in range(epoches):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for batch_x, batch_y, valid_lens in train_loader:\n",
    "                batch_x = batch_x.view(-1, 24, 15).to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                pred = model(batch_x, valid_lens)\n",
    "\n",
    "                loss = criterion(pred, batch_y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            epoch_train_losses.append(avg_loss)\n",
    "            test_avg_loss, _, _ = evaluate(model, criterion, test_loader, device=device)\n",
    "            epoch_test_losses.append(test_avg_loss)\n",
    "            \n",
    "            t.set_postfix(loss=avg_loss, test_loss=test_avg_loss)\n",
    "            t.update(1)\n",
    "            early_stopping(test_avg_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                model.load_state_dict(torch.load(model_path))\n",
    "                #print('Early stopping')\n",
    "                break\n",
    "\n",
    "# Define sigmoid function                \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#  a function to compute a binary classification indicator \n",
    "def metrics(all_preds, all_labels, threshold=0.5):\n",
    "    all_probs = sigmoid(np.array(all_preds))\n",
    "    binary_preds = (all_probs > threshold).astype(int)\n",
    "    conf_matrix = confusion_matrix(all_labels, binary_preds)\n",
    "    accuracy = accuracy_score(all_labels, binary_preds)\n",
    "    sensitivity = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    \n",
    "    return accuracy, sensitivity, specificity, auc\n",
    "\n",
    "# Define CNNLayer\n",
    "class CNNLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding,dropout_rate):\n",
    "        super(CNNLayer, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)  \n",
    "        self.adaptive_maxpool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "    \n",
    "        x = self.dropout(x)  \n",
    "        x = self.adaptive_maxpool(x)\n",
    "        return x\n",
    "\n",
    "#Define the dot product scoring function    \n",
    "class DotProductScore(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DotProductScore, self).__init__()\n",
    "        self.q = nn.Parameter(torch.empty(size=(hidden_size, 1), dtype=torch.float32))\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \n",
    "        initrange = 0.5\n",
    "        self.q.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        scores = torch.matmul(inputs, self.q)\n",
    "        scores = scores.squeeze(-1)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "# Define Attention  \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.scores = DotProductScore(hidden_size)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        scores = self.scores(X)\n",
    "        arrange = torch.arange(X.size(1), dtype=torch.float32, device=X.device).unsqueeze(0)\n",
    "        mask = (arrange < valid_lens.unsqueeze(-1)).float()\n",
    "        scores = scores * mask - (1 - mask) * 1e9\n",
    "        attention_weights = nn.functional.softmax(scores, dim=-1)  \n",
    "        out = torch.matmul(attention_weights.unsqueeze(1), X).squeeze(1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Define  AutoBILSTMTFCNY model   \n",
    "class AutoBILSTMTFCNY(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout, ins_num):\n",
    "        super(AutoBILSTMTFCNY, self).__init__()\n",
    "        self.ins_num = ins_num\n",
    "        \n",
    "        self.cnn1 = CNNLayer(input_size, hidden_size * 2, kernel_size=8, stride=1, padding=3,dropout_rate=dropout)\n",
    "        self.lstm = nn.LSTM(hidden_size * 2, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.attention = Attention(hidden_size * 2)\n",
    "        self.cnn2 = CNNLayer(hidden_size * 2, hidden_size * 4, kernel_size=3, stride=1, padding=1,dropout_rate=dropout)  \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size * 4, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, seq, valid_lens):\n",
    "\n",
    "        out = seq.permute(0, 2, 1)\n",
    "\n",
    "        out = self.cnn1(out)\n",
    "\n",
    "        out = out.permute(0, 2, 1) \n",
    "\n",
    "        output, _ = self.lstm(out)\n",
    "\n",
    "        valid_lens = valid_lens.view(-1,).to(device)\n",
    "\n",
    "        out = self.attention(output, valid_lens)\n",
    "\n",
    "        out = out.unsqueeze(2)\n",
    "\n",
    "        out = self.cnn2(out)\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        out = out.view(-1, self.ins_num, 1)  \n",
    "\n",
    "        out = out.mean(dim=1) \n",
    "\n",
    "        return out\n",
    "\n",
    "# Setting model parameters     \n",
    "def init_model():\n",
    "    input_size = 15  # Input feature dimensions\n",
    "    hidden_size = 30  # Hide layer size\n",
    "    num_layers = 3  # of layers in Bi-LSTM stacks\n",
    "    output_size = 1  # Output size\n",
    "    ins_num = 100  # 100 TCR sequences\n",
    "    dropout = 0.6 # Dropout ratio\n",
    "    \n",
    "    return AutoBILSTMTFCNY(input_size, hidden_size, output_size, num_layers, dropout, ins_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e4b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce an early stop mechanism\n",
    "sys.path.append('../')\n",
    "from python_codes.pytorchtools import EarlyStopping\n",
    "\n",
    "# Read the amino acid characterization file\n",
    "aa_file = \"../Data/PCA15.txt\"\n",
    "aa_vectors = get_features(aa_file)  \n",
    "\n",
    "# 5-fold cross-validation\n",
    "k_fold = 5\n",
    "kf = KFold(n_splits=k_fold, shuffle=True,random_state=42)\n",
    "\n",
    "BATCH_SIZE = 64   # Batch size\n",
    "NUM_EPOCHES = 2000 #Total number of training rounds\n",
    "PATIENCE = 300    # Set the patience value for early stops\n",
    "\n",
    "all_accuracies = []  \n",
    "all_sensitivities = []  \n",
    "all_specificities = []  \n",
    "all_aucs = [] \n",
    "\n",
    "device = \"cuda\"\n",
    "disease_list = [\"RA\", \"T1D\", \"MS\", \"IAA\",\"GBS\",\"JIA\",\"Narcolepsy\"]\n",
    "results = []\n",
    "results_ROC = []\n",
    "\n",
    "for disease_name in disease_list:\n",
    "    data_dir = f'../Data/{disease_name}'\n",
    "    training_data, training_labels = load_data(data_dir)\n",
    "    print(f\"Working on {disease_name} dataset: {len(training_data)} samples\")\n",
    "    \n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    \n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(training_data)):\n",
    "        train_data = [training_data[i] for i in train_idx]\n",
    "        train_labels = [training_labels[i] for i in train_idx]\n",
    "        test_data = [training_data[i] for i in test_idx]\n",
    "        test_labels = [training_labels[i] for i in test_idx]\n",
    "        \n",
    "        \n",
    "        # After the training and test sets are fixed, the training set is then divided into a training set and a validation set\n",
    "        train_data, valid_data, train_labels, valid_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=1234)\n",
    "\n",
    "        \n",
    "        train_input_batch, train_label_batch, train_valid_lens_batch = generate_input(train_data, train_labels, aa_vectors, 15, 100, 24)\n",
    "        valid_input_batch, valid_label_batch, valid_valid_lens_batch = generate_input(valid_data, valid_labels, aa_vectors, 15, 100, 24)\n",
    "        test_input_batch, test_label_batch, test_valid_lens_batch = generate_input(test_data, test_labels, aa_vectors, 15, 100, 24)\n",
    "        \n",
    "        train_dataset = Data.TensorDataset(train_input_batch, train_label_batch, train_valid_lens_batch)\n",
    "        valid_dataset = Data.TensorDataset(valid_input_batch, valid_label_batch, valid_valid_lens_batch)\n",
    "        test_dataset = Data.TensorDataset(test_input_batch, test_label_batch, test_valid_lens_batch)\n",
    "\n",
    "        train_loader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        valid_loader = Data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        test_loader = Data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model = init_model().to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "        train(fold, model, criterion, optimizer, train_loader, valid_loader, epoches=NUM_EPOCHES, device=device)\n",
    "\n",
    "\n",
    "        _, preds, labels = evaluate(model, criterion, test_loader, device=device)\n",
    "        all_preds += preds\n",
    "        all_labels += labels\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    accuracy, sensitivity, specificity, auc = metrics(all_preds, all_labels)\n",
    "    print(f\"Mean Accuracy ({disease_name}): {accuracy:.4f}\")\n",
    "    print(f\"Mean Sensitivity ({disease_name}): {sensitivity:.4f}\")\n",
    "    print(f\"Mean Specificity ({disease_name}): {specificity:.4f}\")\n",
    "    print(f\"Mean AUC ({disease_name}): {auc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'disease': disease_name,\n",
    "        'accuracy': accuracy,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'auc': auc\n",
    "    })\n",
    "\n",
    "    results_ROC.append({\n",
    "        'disease': disease_name,\n",
    "        'auc': auc,\n",
    "        'all_preds': all_preds,\n",
    "        'all_labels': all_labels\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
