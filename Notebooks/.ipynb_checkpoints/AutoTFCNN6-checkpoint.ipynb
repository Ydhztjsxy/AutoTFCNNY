{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08fdaa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, auc, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define functions to read TSV files\n",
    "def read_tsv(filename, inf_ind, skip_1st=False, file_encoding=\"utf8\"):\n",
    "    extract_inf = []\n",
    "    with open(filename, \"r\", encoding=file_encoding) as tsv_f:\n",
    "        if skip_1st:\n",
    "            tsv_f.readline()\n",
    "        line = tsv_f.readline()\n",
    "        while line:\n",
    "            line_list = line.strip().split(\"\\t\")\n",
    "            temp_inf = [line_list[ind] for ind in inf_ind]\n",
    "            extract_inf.append(temp_inf)\n",
    "            line = tsv_f.readline()\n",
    "    return extract_inf\n",
    "\n",
    "# Define a function that reads an amino acid feature file and generates a feature dictionary\n",
    "def get_features(filename, f_num=15):\n",
    "    f_list = read_tsv(filename, list(range(16)), True)\n",
    "    f_dict = {}\n",
    "    left_num = 0\n",
    "    right_num = 0\n",
    "    if f_num > 15:\n",
    "        left_num = (f_num - 15) // 2\n",
    "        right_num = f_num - 15 - left_num\n",
    "    for f in f_list:\n",
    "        f_dict[f[0]] = [0] * left_num + [float(x) for x in f[1:]] + [0] * right_num\n",
    "    f_dict[\"X\"] = [0] * f_num\n",
    "    return f_dict\n",
    "\n",
    "# Defining Input Functions\n",
    "def generate_input(sps, sp_lbs, feature_dict, feature_num, ins_num, max_len):\n",
    "    xs, ys = [], []\n",
    "    i = 0\n",
    "    for sp in sps:\n",
    "        xs.append([[[0] * feature_num] * max_len] * ins_num)\n",
    "        ys.append(sp_lbs[i])\n",
    "        j = 0\n",
    "        for tcr in sp:\n",
    "            tcr_seq = tcr[0]\n",
    "            right_num = max_len - len(tcr_seq)\n",
    "            tcr_seq += \"X\" * right_num\n",
    "            tcr_matrix = []\n",
    "            for aa in tcr_seq:\n",
    "                tcr_matrix.append(feature_dict[aa.upper()])\n",
    "            xs[i][j] = tcr_matrix\n",
    "            j += 1\n",
    "        i += 1\n",
    "    xs = np.array(xs)\n",
    "    xs = xs.swapaxes(2, 3)\n",
    "    ys = np.array(ys)\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "#Define the Generate Label function\n",
    "def load_data(sample_dir):\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    for sample_file in os.listdir(sample_dir):\n",
    "        training_data.append(read_tsv(os.path.join(sample_dir, sample_file), [0, 1], True))\n",
    "        if \"P\" in sample_file:\n",
    "            training_labels.append(1)\n",
    "        elif \"H\" in sample_file:\n",
    "            training_labels.append(0)\n",
    "        else:\n",
    "            print(\"Wrong sample filename! Please name positive samples with 'P' and negative samples with 'H'.\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "    return training_data, training_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f525ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the evaluation function\n",
    "def evaluate(model, criterion, test_loader, device=\"cuda\"):\n",
    "    test_total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_batch_x, test_batch_y in test_loader:\n",
    "            test_batch_x = test_batch_x.to(device)\n",
    "            test_batch_y = test_batch_y.to(device).view(-1, 1)  # 使标签的形状与模型输出匹配\n",
    "            test_pred = model(test_batch_x)\n",
    "\n",
    "            test_loss = criterion(test_pred, test_batch_y)\n",
    "            test_total_loss += test_loss.item()\n",
    "            all_preds.append(test_pred.cpu().numpy())\n",
    "            all_labels.append(test_batch_y.cpu().numpy())\n",
    "            \n",
    "        test_avg_loss = test_total_loss / len(test_loader)\n",
    "        return test_avg_loss, all_preds, all_labels\n",
    "    \n",
    "#Define the training function    \n",
    "from tqdm import tqdm\n",
    "def train(fold, model, criterion, optimizer, train_loader, valid_loader, epoches=100, device=\"cuda\"):\n",
    "    \n",
    "    \n",
    "    model_path = f'../model/AutoY/{disease_name}checkpoint{fold}.pt'  # Save path of the model file\n",
    "    early_stopping = EarlyStopping(PATIENCE, path=model_path, verbose=False)\n",
    "    \n",
    "\n",
    "    epoch_train_losses = []\n",
    "    epoch_test_losses = []\n",
    "    with tqdm(total=epoches) as t:\n",
    "        t.set_description(f'{disease_name} - Fold {fold}')  \n",
    "        for epoch in range(epoches):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device).view(-1, 1)  # 使标签的形状与模型输出匹配\n",
    "                pred = model(batch_x)\n",
    "\n",
    "                loss = criterion(pred, batch_y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "           \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            epoch_train_losses.append(avg_loss)\n",
    "            test_avg_loss, _, _ = evaluate(model, criterion, test_loader, device)\n",
    "            epoch_test_losses.append(test_avg_loss) \n",
    "            t.set_postfix(loss=avg_loss, test_loss=test_avg_loss)\n",
    "            t.update(1)\n",
    "            early_stopping(test_avg_loss, model)\n",
    "            \n",
    "            if early_stopping.early_stop:\n",
    "                model.load_state_dict(torch.load(model_path))\n",
    "                #print('Early stopping')\n",
    "                break\n",
    "                \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))     \n",
    "\n",
    "# Define a function to compute a binary classification indicator               \n",
    "def metrics(all_preds, all_labels, threshold=0.5):\n",
    "    \n",
    "    all_probs = sigmoid(np.array(all_preds))\n",
    "    binary_preds = (all_probs > threshold).astype(int)\n",
    "    conf_matrix = confusion_matrix(all_labels, binary_preds)\n",
    "    accuracy = accuracy_score(all_labels, binary_preds)\n",
    "    sensitivity = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])\n",
    "    specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    \n",
    "    return accuracy, sensitivity, specificity, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0432aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiKernelCNNLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernels):\n",
    "        super(MultiKernelCNNLayer, self).__init__()\n",
    "        self.branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=k, stride=1, padding=k//2),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveMaxPool1d(1)\n",
    "            ) for k in kernels\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [branch(x) for branch in self.branches]\n",
    "        # 在拼接之前打印每个分支的输出尺寸，确保它们是一致的\n",
    "        for i, output in enumerate(outputs):\n",
    "            print(f\"Branch {i} output shape: {output.shape}\")\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.3, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # 初始化 dropout 层\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 生成位置索引并扩展维度以便后续计算\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # 计算分母部分的值\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        # 初始化位置编码矩阵\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        # 计算位置编码的正弦部分\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        # 计算位置编码的余弦部分\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        # 注册为模型的缓冲区\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将位置编码添加到输入中\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        # 应用 dropout 并返回结果\n",
    "        return self.dropout(x)\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, feature_num, hidden_size, output_size, num_heads, num_layers, dropout, max_len, ins_num):\n",
    "        super(TransformerModel, self).__init__()\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, feature_num, hidden_size, output_size, num_heads, num_layers, dropout, max_len, ins_num):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ins_num = ins_num\n",
    "\n",
    "        self.input_embedding = nn.Linear(feature_num, hidden_size)\n",
    "        self.pos_encoder = PositionalEncoding(hidden_size, dropout, max_len)\n",
    "        self.cnn1 = MultiKernelCNNLayer(hidden_size, hidden_size * 4, [2, 3, 4, 5])  # 四倍是因为我们有四个不同的卷积核\n",
    "        # 调整src的维度以匹配cnn_output\n",
    "        self.dim_matcher = nn.Linear(hidden_size, hidden_size * 4)  # 根据实际需要调整维度\n",
    "        self.encoder_layers = nn.TransformerEncoderLayer(d_model=hidden_size * 4, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layers, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.decoder = nn.Linear(hidden_size * 4, output_size)  # 四倍hidden_size作为输入\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.input_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.flatten(0, 1)\n",
    "        src = src.permute(2, 0, 1)\n",
    "        src = self.input_embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        src = src.permute(1, 2, 0)\n",
    "        cnn_output = self.cnn1(src)\n",
    "        cnn_output = cnn_output.permute(2, 0, 1)\n",
    "        # 调整src的维度以匹配cnn_output\n",
    "        src_matched = self.dim_matcher(src)\n",
    "        # 执行残差连接\n",
    "        output = self.transformer_encoder(cnn_output + src_matched)\n",
    "\n",
    "\n",
    "        output = output.permute(1, 0, 2)\n",
    "        output = output.mean(dim=1)\n",
    "        output = self.dropout(output)\n",
    "        output = self.decoder(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n",
    "def init_model():\n",
    "    # 设定模型参数\n",
    "    feature_num = 15  # 输入特征维度\n",
    "    hidden_size = 30  # 隐藏层大小，这是输入嵌入和位置编码后的大小\n",
    "    output_size = 1   # 输出大小，根据你的任务，这可能是分类的类别数或其他\n",
    "    num_heads = 60    # 注意力头数\n",
    "    num_layers = 2    # Transformer编码器层数\n",
    "    dropout = 0.6     # Dropout比率\n",
    "    max_len = 24      # 序列最大长度，这与位置编码有关\n",
    "    ins_num = 100     # 实例数量，这是输入序列的数量\n",
    "\n",
    "    # 初始化模型并将其移到相应的设备上\n",
    "    model = TransformerModel(feature_num, hidden_size, output_size, num_heads, num_layers, dropout, max_len, ins_num)\n",
    "\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9a72377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on RA dataset: 300 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RA - Fold 0:   0%|                                                                            | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 0 output shape: torch.Size([19200, 120, 1])\n",
      "Branch 1 output shape: torch.Size([19200, 120, 1])\n",
      "Branch 2 output shape: torch.Size([19200, 120, 1])\n",
      "Branch 3 output shape: torch.Size([19200, 120, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (576000x24 and 30x120)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m  criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()  \u001b[38;5;66;03m# 使用BCEWithLogitsLoss\u001b[39;00m\n\u001b[0;32m     77\u001b[0m  optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m  \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Final evaluation on test set\u001b[39;00m\n\u001b[0;32m     82\u001b[0m  _, preds, labels \u001b[38;5;241m=\u001b[39m evaluate(model, criterion, test_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[18], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(fold, model, criterion, optimizer, train_loader, valid_loader, epoches, device)\u001b[0m\n\u001b[0;32m     39\u001b[0m batch_x \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     40\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 使标签的形状与模型输出匹配\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, batch_y)\n\u001b[0;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[21], line 94\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, src)\u001b[0m\n\u001b[0;32m     92\u001b[0m cnn_output \u001b[38;5;241m=\u001b[39m cnn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# 调整src的维度以匹配cnn_output\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m src_matched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_matcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# 执行残差连接\u001b[39;00m\n\u001b[0;32m     96\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(cnn_output \u001b[38;5;241m+\u001b[39m src_matched)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (576000x24 and 30x120)"
     ]
    }
   ],
   "source": [
    "# Introduce an early stop mechanism\n",
    "sys.path.append('../')\n",
    "from python_codes.pytorchtools import EarlyStopping\n",
    "\n",
    "# Reading amino acid profile files\n",
    "aa_file = \"../AutoData214/PCA15.txt\"\n",
    "aa_vectors = get_features(aa_file)  \n",
    "\n",
    "# 5-fold cross-validation\n",
    "k_fold = 5\n",
    "kf = KFold(n_splits=k_fold, shuffle=True,random_state=42)\n",
    "\n",
    "BATCH_SIZE = 64   # Batch size\n",
    "NUM_EPOCHES = 2000 #Total number of training rounds\n",
    "PATIENCE = 300    # Set the patience value for early stops\n",
    "\n",
    "all_accuracies = []  \n",
    "all_sensitivities = [] \n",
    "all_specificities = [] \n",
    "all_aucs = []  \n",
    "\n",
    "device = \"cuda\"\n",
    "# Four autoimmune diseases\n",
    "disease_list = [\"RA\", \"T1D\", \"MS\", \"IAA\"]\n",
    "#disease_list = [\"Test\"]\n",
    "results = []\n",
    "results_ROC = []\n",
    "\n",
    "for disease_name in disease_list:\n",
    "    data_dir = f'../AutoData214/{disease_name}'   #Disease File Path\n",
    "    training_data, training_labels = load_data(data_dir)\n",
    "    print(f\"Working on {disease_name} dataset: {len(training_data)} samples\")\n",
    "\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    " \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(training_data)):\n",
    "        train_data = [training_data[i] for i in train_idx]\n",
    "        train_labels = [training_labels[i] for i in train_idx]\n",
    "        test_data = [training_data[i] for i in test_idx]\n",
    "        test_labels = [training_labels[i] for i in test_idx]\n",
    "        \n",
    "        \n",
    "        # After the training and test sets are fixed, the training set is then divided into a training set and a validation set\n",
    "        train_data, valid_data, train_labels, valid_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=1234)\n",
    "\n",
    "                                                                        \n",
    "        train_input_batch, train_label_batch = generate_input(train_data, train_labels, aa_vectors, 15, 100, 24)\n",
    "        \n",
    "        train_input_batch, train_label_batch = torch.Tensor(train_input_batch).to(torch.device(\"cuda\")), torch.LongTensor(train_label_batch).to(torch.device(\"cuda\"))\n",
    "\n",
    "        valid_input_batch, valid_label_batch = generate_input(valid_data, valid_labels, aa_vectors, 15, 100, 24)\n",
    "        valid_input_batch, valid_label_batch = torch.Tensor(valid_input_batch).to(torch.device(\"cuda\")), torch.LongTensor(valid_label_batch).to(torch.device(\"cuda\"))\n",
    "        \n",
    "        test_input_batch, test_label_batch = generate_input(test_data, test_labels, aa_vectors, 15, 100, 24)\n",
    "        test_input_batch, test_label_batch = torch.Tensor(test_input_batch).to(torch.device(\"cuda\")), torch.LongTensor(test_label_batch).to(torch.device(\"cuda\"))\n",
    "        \n",
    "        \n",
    "        # 在数据加载部分，确保标签是浮点数\n",
    "        train_label_batch = train_label_batch.float()\n",
    "        valid_label_batch = valid_label_batch.float()\n",
    "        test_label_batch = test_label_batch.float()\n",
    "\n",
    "        train_dataset = Data.TensorDataset(train_input_batch, train_label_batch)\n",
    "        valid_dataset = Data.TensorDataset(valid_input_batch, valid_label_batch)\n",
    "        test_dataset = Data.TensorDataset(test_input_batch, test_label_batch)\n",
    "        \n",
    "        train_loader = Data.DataLoader(train_dataset, len(train_input_batch), True)\n",
    "        valid_loader = Data.DataLoader(valid_dataset, len(valid_input_batch), True)\n",
    "        test_loader = Data.DataLoader(test_dataset, len(test_input_batch), True)\n",
    "\n",
    "\n",
    "        model = init_model().to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()  # 使用BCEWithLogitsLoss\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "        train(fold, model, criterion, optimizer, train_loader, valid_loader, epoches=NUM_EPOCHES, device=device)\n",
    "\n",
    "       # Final evaluation on test set\n",
    "        _, preds, labels = evaluate(model, criterion, test_loader, device=device)\n",
    "        all_preds += preds\n",
    "        all_labels += labels\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    accuracy, sensitivity, specificity, auc = metrics(all_preds, all_labels)\n",
    "    print(f\"Mean Accuracy ({disease_name}): {accuracy:.4f}\")\n",
    "    print(f\"Mean Sensitivity ({disease_name}): {sensitivity:.4f}\")\n",
    "    print(f\"Mean Specificity ({disease_name}): {specificity:.4f}\")\n",
    "    print(f\"Mean AUC ({disease_name}): {auc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'disease': disease_name,\n",
    "        'accuracy': accuracy,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'auc': auc\n",
    "    })\n",
    "\n",
    "    results_ROC.append({\n",
    "        'disease': disease_name,\n",
    "        'auc': auc,\n",
    "        'all_preds': all_preds,\n",
    "        'all_labels': all_labels\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067acef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f917b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
